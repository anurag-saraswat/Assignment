# -*- coding: utf-8 -*-
"""ML2_Assignment_2_Question_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_GLzVcQ9aTiNY_EPxdJVdtOkmD68hmdW

#  1. Use ​ CIFAR-10 ​ dataset, [80 marks + 20 Bonus]
a. Implement a ​ 6-layer CNN network (Choose your own architecture) to
perform classification, use the same training and testing data split as
given in the dataset, Report the following

i.
Accuracy on test data by varying the ​ Optimization Techniques
1. Vanilla SGD [5 marks]
2. SGD with momentum. [5 marks]
3. Adam [5 marks]

ii.
Accuracy on test data by varying the ​ Normalization Techniques
1. Dropout [10 marks]
2. Batch Normalization. [10 marks]

iii.
Accuracy on test data by varying ​ activation function ​ in between
CNN layers​ ,
1. Identity. [5 marks]
2. Sigmoid. [5 marks]
3. ReLU. [5 marks]
4. Tanh. [5 marks]

iv.
Accuracy on test data by varying the ​ loss functions
1. Cross-Entropy Loss [5 marks]
2. MSELoss [5 marks]
3. L1 Loss [5 marks]
"""

# Importing library


import numpy as np
import matplotlib.pyplot as plt

import torch
import torchvision
import torch.optim as optim
import torchvision.transforms as transforms

"""## Data Visualization"""

# Downloading and Loading Data Set

transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])

trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=10,shuffle=True, num_workers=2)

testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)
testloader = torch.utils.data.DataLoader(testset, batch_size=10, shuffle=False, num_workers=2)

classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')

# Visualising 10 random sample from training data
indexes = np.random.randint(0, trainset.data.shape[0], size=10)

plt.figure(figsize=(8, 8))

for i in range(len(indexes)):
    sub = plt.subplot(2, 5, i + 1)
    image = trainset.data[indexes[i]]
    plt.imshow(image, cmap='gray')
    plt.axis('off')
    sub.set_title("Label : " + str(classes[int(trainset.targets[indexes[i]])]))
plt.show()
plt.close('all')

"""## CNN Model

6 Convolution Layer and 4 Fully Connected Layer
"""

import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6
        #self.dropout = nn.Dropout2d(p=0.2)

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8
        

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.BatchNorm(F.relu(self.conv5(x)))
        x = F.relu(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.relu(self.fc1(x))
        x = self.BatchNorm1(F.relu(self.fc2(x)))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# Initializing GPU
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

net = Net()
net.to(device)

from torchsummary import summary
summary(net,(3, 32, 32))



"""## Accuracy on test data by varying the ​ Optimization Techniques
1. Vanilla SGD 
2. SGD with momentum
3. Adam 
"""

# Vanilla SGD
net = Net()
net.to(device)

Acc_V_SGD = []
loss_V_SGD = []
epoch = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999:    # print every 2000 mini-batches
            #print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 2000))
            loss_V_SGD.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_V_SGD.append(temp)
    print('%d Epoch Complete...'%epoch)
print('Finished Training Vanilla SGD')

# SGD with momentum
del net
net = Net()
net.to(device)

Acc_M_SGD = []
loss_M_SGD = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data[0].to(device), data[1].to(device)
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_M_SGD.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_M_SGD.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training SGD with Momentum')

# Using Adam

del net
net = Net()
net.to(device)

Acc_ADAM = []
loss_ADAM = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_ADAM.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_ADAM.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training ADAM Optimization')

#Result Comparision in Accuracy Achieved using different optimization strategies

plt.style.use("seaborn")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Comparison of Accuracy achieved per Epoch")

x = [i for i in range(1,21)]


plt.plot(x,Acc_V_SGD,label="Vanilla SGD")
plt.plot(x,Acc_M_SGD,label="SGD with Momontum")
plt.plot(x,Acc_ADAM,label="ADAM")

plt.legend()
plt.savefig('./Optimization.png')
plt.show()

plt.style.use("seaborn")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("Comparison of Loss per 2000 mini batch")

x = [i for i in range(1,len(loss_V_SGD)+1)]


plt.plot(x,loss_V_SGD,label="Vanilla SGD")
plt.plot(x,loss_M_SGD,label="SGD with Momontum")
plt.plot(x,loss_ADAM,label="ADAM")

plt.legend()
plt.savefig('./Optimization_loss.png')
plt.show()

"""##Accuracy on test data by varying the ​ Normalization Techniques

- Batch Normalization
- Dropout 
"""

# Using Batch Normalization

del net
net = Net()
net.to(device)

from torchsummary import summary
summary(net,(3, 32, 32))


Acc_Batch_Normalization = []
loss_Batch_Normalization = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_Batch_Normalization.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_Batch_Normalization.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Batch Normalization')

# Using Dropout
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6
        self.dropout2 = nn.Dropout2d(p=0.2)
        self.dropout = nn.Dropout(p=0.2)

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8
        
        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16

        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.dropout2(F.relu(self.conv5(x)))
        x = F.relu(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.relu(self.fc1(x))
        x = self.dropout(F.relu(self.fc2(x)))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x
        

del net
net = Net()
net.to(device)

from torchsummary import summary
summary(net,(3, 32, 32))


Acc_Dropout = []
loss_Dropout = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_Dropout.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_Dropout.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Dropout')

#Result Comparision in Accuracy Achieved using different normalization strategies

plt.style.use("seaborn")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Comparison of Accuracy achieved per Epoch")

x = [i for i in range(1,21)]


plt.plot(x,Acc_Batch_Normalization,label="Batch Normalization")
plt.plot(x,Acc_Dropout,label="Dropout")


plt.legend()
plt.savefig('./Normalization.png')
plt.show()

plt.style.use("seaborn")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("Comparison of Loss per 2000 mini batch")

x = [i for i in range(1,len(loss_V_SGD)+1)]


plt.plot(x,loss_Batch_Normalization,label="Batch Normalization")
plt.plot(x,loss_Dropout,label="Dropout")

plt.legend()
plt.savefig('./Normalization_loss.png')
plt.show()

"""## Accuracy on test data by varying ​ activation function ​ in between CNN layers​ ,
1. Identity.
2. Sigmoid.
3. ReLU. 
4. Tanh. 
"""

# Using Relu Activation function


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.BatchNorm(F.relu(self.conv5(x)))
        x = F.relu(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.relu(self.fc1(x))
        x = self.BatchNorm1(F.relu(self.fc2(x)))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

del net
net = Net()
net.to(device)


Acc_Relu = []
loss_Relu = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_Relu.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_Relu.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Relu Activation Function')

# Using Sigmoid Activation function


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.sigmoid(self.conv1(x)))
        x = F.sigmoid(self.conv2(x))
        x = F.sigmoid(self.conv3(x))
        x = F.sigmoid(self.conv4(x))
        x = self.BatchNorm(F.sigmoid(self.conv5(x)))
        x = F.sigmoid(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.sigmoid(self.fc1(x))
        x = self.BatchNorm1(F.sigmoid(self.fc2(x)))
        x = F.sigmoid(self.fc3(x))
        x = self.fc4(x)
        return x

del net
net = Net()
net.to(device)


Acc_Sigmoid = []
loss_Sigmoid = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_Sigmoid.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_Sigmoid.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Sigmoid Activation Function')

# Using Tanh Activation function


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.tanh(self.conv1(x)))
        x = F.tanh(self.conv2(x))
        x = F.tanh(self.conv3(x))
        x = F.tanh(self.conv4(x))
        x = self.BatchNorm(F.tanh(self.conv5(x)))
        x = F.tanh(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.tanh(self.fc1(x))
        x = self.BatchNorm1(F.tanh(self.fc2(x)))
        x = F.tanh(self.fc3(x))
        x = self.fc4(x)
        return x

del net
net = Net()
net.to(device)


Acc_Tanh = []
loss_Tanh = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_Tanh.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_Tanh.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Tanh Activation Function')

# Using Identity Activation function


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(self.conv1(x))
        x = self.conv2(x)
        x = self.conv3(x)
        x = self.conv4(x)
        x = self.BatchNorm(F.tanh(self.conv5(x)))
        x = self.conv6(x)

        x = x.view(-1, 16*13*13)
        x = self.fc1(x)
        x = self.BatchNorm1(F.tanh(self.fc2(x)))
        x = self.fc3(x)
        x = self.fc4(x)
        return x

del net
net = Net()
net.to(device)


Acc_Identity = []
loss_Identity = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_Identity.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_Identity.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Identity Activation Function')

#Result Comparision in Accuracy Achieved using different normalization strategies

plt.style.use("seaborn")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Comparison of Accuracy achieved per Epoch")

x = [i for i in range(1,21)]


plt.plot(x,Acc_Identity,label="Identity")
plt.plot(x,Acc_Relu,label="Relu")
plt.plot(x,Acc_Sigmoid,label="Sigmoid")
plt.plot(x,Acc_Tanh,label="Tanh")


plt.legend()
plt.savefig('./Activation.png')
plt.show()

plt.style.use("seaborn")
plt.xlabel("Iteration")
plt.ylabel("Loss")
plt.title("Comparison of Loss per 2000 mini batch")

x = [i for i in range(1,len(loss_V_SGD)+1)]


plt.plot(x,loss_Identity,label="Identity")
plt.plot(x,loss_Relu,label="Relu")
plt.plot(x,loss_Sigmoid,label="Sigmoid")
plt.plot(x,loss_Tanh,label="Tanh")

plt.legend()
plt.savefig('./Acctivation_loss.png')
plt.show()

"""## iv. Accuracy on test data by varying the ​ loss functions

- Cross-Entropy Loss
- MSELoss
- L1 Loss 
"""

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6
        #self.dropout = nn.Dropout2d(p=0.2)

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8
        

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.BatchNorm(F.relu(self.conv5(x)))
        x = F.relu(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.relu(self.fc1(x))
        x = self.BatchNorm1(F.relu(self.fc2(x)))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# Using Cross-Entrophy Loss
try :
  del net
except : 
  pass

net = Net()
net.to(device)


Acc_Cross_Entrophy = []
loss_Cross_Entrophy = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(15):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_Cross_Entrophy.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_Cross_Entrophy.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Cross Entrophy Loss')

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6
        #self.dropout = nn.Dropout2d(p=0.2)

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8
        

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.BatchNorm(F.relu(self.conv5(x)))
        x = F.relu(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.relu(self.fc1(x))
        x = self.BatchNorm1(F.relu(self.fc2(x)))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return F.softmax(x)

try :
  del net
except : 
  pass

net = Net()
net.to(device)


Acc_MSE = []
loss_MSE = []

criterion = nn.MSELoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(15):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), (data[1].float()).to(device)


        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels.view(-1,1))
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_MSE.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_MSE.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using MSE Loss')

# Using L1 Loss

try :
  del net
except : 
  pass

net = Net()
net.to(device)


Acc_L1 = []
loss_L1 = []

criterion = nn.L1Loss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(15):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), (data[1].float()).to(device)


        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels.view(-1,1))
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_L1.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_L1.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using L1 Loss')

#Result Comparision in Accuracy Achieved using different Loss Function

plt.style.use("seaborn")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Comparison of Accuracy achieved per Epoch")

x = [i for i in range(1,16)]


plt.plot(x,Acc_Cross_Entrophy,label="Cross Entrophy Loss")
plt.plot(x,Acc_MSE,label="MSE Loss")
plt.plot(x,Acc_L1,label="L1 Loss")



plt.legend()
plt.savefig('./LOSSFunction.png')
plt.show()

"""## Find the best configuration for your CNN (a combination of Optimization Technique, Normalization technique, activation function and Loss function),support your claim."""

import torch.nn as nn
import torch.nn.functional as F


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__() 
      
        self.conv1 = nn.Conv2d(3, 6, 3)  # 32x32x3 ---> 30x30x6  

        self.pool = nn.MaxPool2d(2, 2)   # 30x30x6 ---> 15x15x6
        #self.dropout = nn.Dropout2d(p=0.2)

        self.conv2 = nn.Conv2d(6, 8, 3)  # 15x15x6 ---> 13x13x8
        

        self.conv3 = nn.Conv2d(8 , 10, 1) # 13x13x8 ---> 13x13x10

        self.conv4 = nn.Conv2d(10, 12, 1) # 13x13x10 ---> 13x13x12

        self.conv5 = nn.Conv2d(12, 14, 1) # 13x13x12 ---> 13x13x14
        
        self.BatchNorm = nn.BatchNorm2d(14)
        self.conv6 = nn.Conv2d(14, 16, 1) # 13x13x14 ---> 13x13x16
    


        self.fc1 = nn.Linear(16 * 13 * 13, 1024)
        self.fc2 = nn.Linear(1024, 512)
        self.BatchNorm1 = nn.BatchNorm1d(512)
        self.fc3 = nn.Linear(512, 256)
        self.fc4 = nn.Linear(256, 10)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = F.relu(self.conv2(x))
        x = F.relu(self.conv3(x))
        x = F.relu(self.conv4(x))
        x = self.BatchNorm(F.relu(self.conv5(x)))
        x = F.relu(self.conv6(x))

        x = x.view(-1, 16*13*13)
        x = F.relu(self.fc1(x))
        x = self.BatchNorm1(F.relu(self.fc2(x)))
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

del net
net = Net()
net.to(device)

from torchsummary import summary
summary(net,(3, 32, 32))


Acc_ = []
loss_ = []

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(net.parameters(), lr=0.001)

for epoch in range(20):  # loop over the dataset multiple times

    running_loss = 0.0
    for i, data in enumerate(trainloader, 0):
        # get the inputs; data is a list of [inputs, labels]
        #inputs, labels = data
        inputs, labels = data[0].to(device), data[1].to(device)

        # zero the parameter gradients
        optimizer.zero_grad()

        # forward + backward + optimize
        outputs = net(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        # print statistics
        running_loss += loss.item()
        if i % 2000 == 1999: 
            loss_.append(float(running_loss / 2000))
            running_loss = 0.0

    correct = 0
    total = 0
    with torch.no_grad():
        for data in testloader:
            images, labels = data[0].to(device), data[1].to(device)
            outputs = net(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()
    
    temp = (100 * correct / total)
    Acc_.append(temp)
    print('%d Epoch Complete...'%(epoch+1))
print('Finished Training Using Best Model')

#Result Comparision in Accuracy Achieved using Best Combination.

plt.style.use("seaborn")
plt.xlabel("Epoch")
plt.ylabel("Accuracy")
plt.title("Comparison of Accuracy achieved per Epoch")

x = [i for i in range(1,21)]


plt.plot(x,Acc_)

plt.savefig('./Best.png')
plt.show()

